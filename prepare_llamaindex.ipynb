{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-Index Knowledge Based \n",
    "[tutorial](https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tree Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 10701 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 10701 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 10701 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader, StorageContext\n",
    "from IPython.display import Markdown, display\n",
    "storage_context = StorageContext.from_defaults()\n",
    "persist_dir = \"./tree\"\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "new_index = GPTTreeIndex.from_documents(documents, storage_context=storage_context)\n",
    "storage_context.persist(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')).\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [3]/[3]\n",
      ">[Level 0] Selected node: [3]/[3]\n",
      ">[Level 0] Selected node: [3]/[3]\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 1] Selected node: [7]/[7]\n",
      ">[Level 1] Selected node: [7]/[7]\n",
      ">[Level 1] Selected node: [7]/[7]\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 4333 tokens\n",
      "> [retrieve] Total LLM token usage: 4333 tokens\n",
      "> [retrieve] Total LLM token usage: 4333 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "> [retrieve] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 638 tokens\n",
      "> [get_response] Total LLM token usage: 638 tokens\n",
      "> [get_response] Total LLM token usage: 638 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "No, not all sugar-free products are calorie-free. Sugar-free products may still contain calories from other sources such as fat, protein, and carbohydrates. It is important to read the nutrition label to check the calorie content of the product.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = new_index.as_query_engine()\n",
    "response = query_engine.query(\"Are all sugar-free products calorie-free?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNo, not all sugar-free products are calorie-free. Sugar-free products may still contain calories from other sources such as fat, protein, and carbohydrates. It is important to read the nutrition label to check the calorie content of the product.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 23443 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 23443 tokens\n",
      "> [build_index_from_nodes] Total embedding token usage: 23443 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "persist_dir = \"./persist\"\n",
    "vector_store_index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "storage_context.persist(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 16 tokens\n",
      "> [retrieve] Total embedding token usage: 16 tokens\n",
      "> [retrieve] Total embedding token usage: 16 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1818 tokens\n",
      "> [get_response] Total LLM token usage: 1818 tokens\n",
      "> [get_response] Total LLM token usage: 1818 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "No. Carbohydrate foods, particularly starchy foods such as rice, bread, noodles and cereals, form a major component of the body's energy source. All starchy foods</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = vector_store_index.as_query_engine()\n",
    "response = query_engine.query(\"If I have diabetes, does that mean I can never consume starchy foods?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import ResponseSynthesizer, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('data', exclude=['raw_data_combined.txt']).load_data()\n",
    "docs_head = documents[:50]\n",
    "docs_tail = documents[50:]\n",
    "vector_store_index = GPTVectorStoreIndex.from_documents(docs_head, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "vector_store_index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 140 tokens\n",
      "> [retrieve] Total embedding token usage: 140 tokens\n",
      "> [retrieve] Total embedding token usage: 140 tokens\n",
      "> [retrieve] Total embedding token usage: 140 tokens\n",
      "> [retrieve] Total embedding token usage: 140 tokens\n",
      "> [retrieve] Total embedding token usage: 140 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 251 tokens\n",
      "> [get_response] Total LLM token usage: 251 tokens\n",
      "> [get_response] Total LLM token usage: 251 tokens\n",
      "> [get_response] Total LLM token usage: 251 tokens\n",
      "> [get_response] Total LLM token usage: 251 tokens\n",
      "> [get_response] Total LLM token usage: 251 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n",
      "> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "No, you do not get diabetes from eating sweets. Eating too much sugar can increase your risk of developing diabetes, but it is not the direct cause. Eating a balanced diet and exercising regularly can help reduce your risk of developing diabetes.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.indices.query.query_transform.base import HyDEQueryTransform\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "from llama_index.vector_stores.types import (\n",
    "    VectorStoreQuery,\n",
    "    VectorStoreQueryMode,\n",
    ")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_store_index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.85)\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "response = query_engine.query('Do I get diabetes from sweets?')\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
